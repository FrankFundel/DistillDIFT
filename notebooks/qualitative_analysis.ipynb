{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from utils.visualization import display_image_pair\n",
    "from utils.model import read_model_config, load_model\n",
    "from utils.dataset import read_dataset_config, load_dataset, Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_type = 'layers' # 'layers', 'timesteps', 'combination', 'dino'\n",
    "\n",
    "if analysis_type == 'layers':\n",
    "    model_name = 'diff_add_analysis_layers'\n",
    "elif analysis_type == 'timesteps':\n",
    "    model_name = 'diff_add_analysis_timesteps'\n",
    "elif analysis_type == 'combination':\n",
    "    model_name = 'combination_add_analysis'\n",
    "elif analysis_type == 'dino':\n",
    "    model_name = 'dinov2_analysis'\n",
    "\n",
    "#model_name = \"dit_add_analysis_layers\" # REMOVE LATER\n",
    "#model_name = \"ijepa\" # REMOVE LATER\n",
    "\n",
    "dataset_config = '../dataset_config.yaml'\n",
    "model_config = '../eval_config.yaml'\n",
    "device_type = 'cuda'\n",
    "pck_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config\n",
    "model_config = read_model_config(model_config)[model_name]\n",
    "model_config['output_all'] = True # for combination_add_analysis\n",
    "\n",
    "# Get model parameters\n",
    "image_size = model_config.get('image_size', (768, 768))\n",
    "grad_enabled = model_config.get('grad_enabled', False)\n",
    "rescale_data = model_config.get('rescale_data', False)\n",
    "image_range = model_config.get('image_range', (-1, 1))\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_name, model_config)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(device_type)\n",
    "model.to(device)\n",
    "\n",
    "# Load dataset config\n",
    "dataset_config = read_dataset_config(dataset_config)\n",
    "\n",
    "# Define preprocessor\n",
    "preprocess = Preprocessor(image_size, image_range=image_range, rescale_data=rescale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "dataset_name = \"S2K\"\n",
    "print(f\"Evaluating dataset: {dataset_name}\")\n",
    "config = dataset_config[dataset_name]\n",
    "config['only_non_unique'] = True\n",
    "dataset = load_dataset(dataset_name, config)\n",
    "\n",
    "# Take first sample\n",
    "unprocessed_sample = dataset[7]\n",
    "\n",
    "# Visualize sample\n",
    "display_image_pair(unprocessed_sample, show_bbox=True)\n",
    "\n",
    "# Preprocess sample\n",
    "sample = preprocess(copy.deepcopy(unprocessed_sample))\n",
    "\n",
    "def batchify(sample):\n",
    "    batch = {}\n",
    "    for key in sample:\n",
    "        if key in ['source_image', 'target_image', 'source_bbox', 'target_bbox']:\n",
    "            batch[key] = sample[key].unsqueeze(0)\n",
    "        else:\n",
    "            batch[key] = [sample[key]]\n",
    "    return batch\n",
    "\n",
    "# Batchify sample\n",
    "batch = batchify(sample)\n",
    "\n",
    "# load images on device\n",
    "batch['source_image'] = batch['source_image'].to(device)\n",
    "batch['target_image'] = batch['target_image'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through model\n",
    "with torch.set_grad_enabled(grad_enabled):\n",
    "    source_features = model.get_features(batch['source_image'], batch['source_category'])\n",
    "    target_features = model.get_features(batch['target_image'], batch['target_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Co-PCA over different layers and timesteps\n",
    "# Apply Co-PCA to features\n",
    "import numpy as np\n",
    "from kmeans_pytorch import kmeans\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "reduce_method = 'kmeans' # 'copca', 'kmeans'\n",
    "n_cluster = 6 # len(sample['source_parts'])\n",
    "\n",
    "def co_pca(features, n):\n",
    "    b, c, h, w = features.shape\n",
    "    features = features.type(torch.float32).permute(1, 0, 2, 3).reshape(c, -1).T # [c, b*h*w]\n",
    "    \n",
    "    U, S, V = torch.pca_lowrank(features, q=n)\n",
    "    reduced = torch.matmul(features, V[:, :n])\n",
    "    reduced = reduced.T.view(n, b, h, w).permute(1, 0, 2, 3)[:, 1:n, :, :]\n",
    "    reduced = (reduced - reduced.min()) / (reduced.max() - reduced.min())\n",
    "    return reduced\n",
    "\n",
    "def kmeans_with_hungarian(features, n):\n",
    "    num_maps, C, H, W = features.shape\n",
    "    rgb_images = []\n",
    "\n",
    "    # Placeholder for centroids of both maps for Hungarian matching\n",
    "    centroids_list = []\n",
    "    colors = torch.tensor(plt.cm.get_cmap('hsv', n)(range(n))[:, :3] * 255, dtype=torch.uint8).to(device)\n",
    "\n",
    "    for i in range(num_maps):\n",
    "        features_flat = features[i].permute(1, 2, 0).reshape(-1, C)\n",
    "        nonzero_indices = torch.norm(features_flat, dim=1) > 0\n",
    "        nonzero_features = features_flat[nonzero_indices]\n",
    "\n",
    "        cluster_ids, cluster_centroids = kmeans(\n",
    "            X=nonzero_features,\n",
    "            num_clusters=n,\n",
    "            distance='cosine',\n",
    "            iter_limit=100,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        centroids_list.append(cluster_centroids)\n",
    "        \n",
    "        # Generate colors for the clusters\n",
    "        rgb_image = torch.zeros(H * W, 3, dtype=torch.uint8, device=device)\n",
    "        rgb_image[nonzero_indices] = colors[cluster_ids.cpu()]\n",
    "        rgb_images.append(rgb_image.view(H, W, 3))\n",
    "\n",
    "    # Hungarian matching to align clusters between maps\n",
    "    cost_matrix = torch.cdist(centroids_list[0], centroids_list[1], p=2).cpu().numpy()\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    col_ind = np.argsort(col_ind)\n",
    "    \n",
    "    # Convert the second RGB image to a numpy array for easier manipulation\n",
    "    rgb_image2_np = rgb_images[1].cpu().numpy().reshape(-1, 3)\n",
    "\n",
    "    # Initialize a new RGB image for the corrected labels\n",
    "    rgb_image2_corrected = np.zeros_like(rgb_image2_np)\n",
    "\n",
    "    # Apply the Hungarian matching results to reassign colors\n",
    "    for original_label, new_label in zip(row_ind, col_ind):\n",
    "        # Find pixels in the original image that match the original label's color\n",
    "        original_color = colors[original_label].cpu().numpy()\n",
    "        is_original_color = np.all(rgb_image2_np == original_color, axis=1)\n",
    "        # Assign the new color based on the matching\n",
    "        new_color = colors[new_label].cpu().numpy()\n",
    "        rgb_image2_corrected[is_original_color] = new_color\n",
    "\n",
    "    # Convert the corrected image back to a tensor and reshape it to the original dimensions\n",
    "    rgb_images[1] = torch.tensor(rgb_image2_corrected.reshape(H, W, 3), dtype=torch.uint8, device=device)\n",
    "\n",
    "    # Prepare for return\n",
    "    rgb_images = torch.stack(rgb_images).permute(0, 3, 1, 2)  # Shape: [2, 3, H, W]\n",
    "    return rgb_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add concatenation of all layers to the list\n",
    "if analysis_type != 'combination':\n",
    "    source_features.append(torch.cat([interpolate(f, size=(512, 512), mode='bilinear') for f in source_features], dim=1))\n",
    "    target_features.append(torch.cat([interpolate(f, size=(512, 512), mode='bilinear') for f in target_features], dim=1))\n",
    "\n",
    "reduced_features = []\n",
    "for sf, tf in zip(source_features, target_features):\n",
    "    if analysis_type == 'timesteps': # optional\n",
    "        sf = interpolate(sf, size=(512, 512), mode='bilinear')\n",
    "        tf = interpolate(tf, size=(512, 512), mode='bilinear')\n",
    "    sf_masked = sf * interpolate(torch.tensor(sample['source_annotation']['mask']).unsqueeze(0).unsqueeze(0), size=sf.shape[-2:], mode='bilinear').squeeze(0).to(device)\n",
    "    tf_masked = tf * interpolate(torch.tensor(sample['target_annotation']['mask']).unsqueeze(0).unsqueeze(0), size=tf.shape[-2:], mode='bilinear').squeeze(0).to(device)\n",
    "    concatenated = torch.cat((sf_masked, tf_masked), dim=0)\n",
    "    #concatenated = torch.cat((sf, tf), dim=0)\n",
    "    if reduce_method == 'copca':\n",
    "        reduced = co_pca(concatenated, n=4)\n",
    "    elif reduce_method == 'kmeans':\n",
    "        reduced = kmeans_with_hungarian(concatenated, n=n_cluster)\n",
    "    reduced_features.append(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with (num_layers + 1) rows and 3 columns\n",
    "num_layers = len(reduced_features)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=num_layers + 1, figsize=(3 * (num_layers + 1), 6))\n",
    "\n",
    "# Display source and target images in the first row\n",
    "axes[0, 0].imshow(unprocessed_sample['source_image'])\n",
    "axes[1, 0].imshow(unprocessed_sample['target_image'])\n",
    "axes[0, 0].axis('off')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Set titles\n",
    "axes[0, 0].set_title(\"Source Image\")\n",
    "\n",
    "if isinstance(model_config.get('layers', None), list) and num_layers > len(model_config['layers']):\n",
    "    model_config['layers'].append('Concatenated')\n",
    "\n",
    "if isinstance(model_config.get('step', None), list) and num_layers > len(model_config['step']):\n",
    "    model_config['step'].append('Concatenated')\n",
    "\n",
    "# Loop over each layer to display the reduced features\n",
    "for l, features in enumerate(reduced_features):\n",
    "    source_size = unprocessed_sample['source_size'][::-1] # [H, W] -> [W, H]\n",
    "    target_size = unprocessed_sample['target_size'][::-1] # [H, W] -> [W, H]\n",
    "    axes[0, l+1].imshow(interpolate(features[0].unsqueeze(0), size=source_size)[0].permute(1, 2, 0).cpu())\n",
    "    axes[1, l+1].imshow(interpolate(features[1].unsqueeze(0), size=target_size)[0].permute(1, 2, 0).cpu())\n",
    "    if analysis_type == 'layers' or analysis_type == 'dino':\n",
    "        axes[0, l+1].set_title(f\"Layer {model_config['layers'][l]}\")\n",
    "    elif analysis_type == 'timesteps':\n",
    "        axes[0, l+1].set_title(f\"Timestep {model_config['step'][l]}\")\n",
    "    elif analysis_type == 'combination':\n",
    "        axes[0, l+1].set_title(f\"{['SD', 'DINO', 'Combined'][l]} Features\")\n",
    "    axes[0, l+1].axis('off')\n",
    "    axes[1, l+1].axis('off')\n",
    "\n",
    "fig.subplots_adjust(wspace=1.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilldift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
