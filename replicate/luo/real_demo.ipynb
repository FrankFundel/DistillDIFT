{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the demo.** Run the following cells to compute semantic correspondences for real image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from archs.stable_diffusion.diffusion import latent_to_image\n",
    "from archs.stable_diffusion.resnet import collect_dims\n",
    "from archs.correspondence_utils import (\n",
    "  process_image,\n",
    "  rescale_points,\n",
    "  draw_correspondences,\n",
    "  compute_pck,\n",
    "  find_nn_source_correspondences,\n",
    "  find_best_buddies_correspondences,\n",
    "  find_cyclical_correspondences,\n",
    ")\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import einops\n",
    "import math\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision\n",
    "\n",
    "from extract_hyperfeatures import load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e0407b8244a308446e5877e5b1220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/ra63des/.local/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion_mode: inversion\n",
      "idxs: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2)]\n",
      "output_resolution: 64\n",
      "prompt: \n",
      "negative_prompt: \n"
     ]
    }
   ],
   "source": [
    "# Memory requirement is 13731MiB\n",
    "device = \"cuda\"\n",
    "config_path = \"configs/real.yaml\"\n",
    "config, diffusion_extractor, aggregation_network = load_models(config_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pair(ann, load_size, device, image_path=\"\"):\n",
    "  img1_pil = Image.open(f\"{image_path}/{ann['source_path']}\").convert(\"RGB\")\n",
    "  img2_pil = Image.open(f\"{image_path}/{ann['target_path']}\").convert(\"RGB\")\n",
    "  source_size = img1_pil.size\n",
    "  target_size = img2_pil.size\n",
    "\n",
    "  # swap from (x, y) to (y, x)\n",
    "  if \"source_points\" in ann:\n",
    "    source_points, target_points = ann[\"source_points\"], ann[\"target_points\"]\n",
    "    source_points = np.flip(source_points, 1)\n",
    "    target_points = np.flip(target_points, 1)\n",
    "    source_points = rescale_points(source_points, source_size, load_size)\n",
    "    target_points = rescale_points(target_points, target_size, load_size)\n",
    "  else:\n",
    "    source_points, target_points = None, None\n",
    "\n",
    "  img1, img1_pil = process_image(img1_pil, res=load_size)\n",
    "  img2, img2_pil = process_image(img2_pil, res=load_size)\n",
    "  img1, img2 = img1.to(device), img2.to(device)\n",
    "  imgs = torch.cat([img1, img2])\n",
    "  \n",
    "  return source_points, target_points, img1_pil, img2_pil, imgs\n",
    "\n",
    "def load_saliency_pair(ann, output_size, device, saliency_path=\"\"):\n",
    "  def _load_saliency(path):\n",
    "    if os.path.exists(path):\n",
    "      saliency_pil = Image.open(path).convert(\"L\")\n",
    "      saliency_map, _ = process_image(saliency_pil, res=output_size, range=(0, 1))\n",
    "      saliency_map = einops.rearrange(saliency_map, 'b c h w -> (b c) (h w)')\n",
    "    else:\n",
    "      saliency_map = torch.ones((1, output_size[0] * output_size[1]))\n",
    "    saliency_map = saliency_map.to(device)\n",
    "    return saliency_map\n",
    "  saliency_map1 = _load_saliency(f\"{saliency_path}/{ann['source_path']}\")\n",
    "  saliency_map2 = _load_saliency(f\"{saliency_path}/{ann['target_path']}\")\n",
    "  return saliency_map1, saliency_map2\n",
    "\n",
    "def reshape_descriptors(img1_feats, img2_feats):\n",
    "  b, d, w, h = img1_feats.shape\n",
    "  descriptors1 = img1_feats.view((b, d, -1)).permute((0, 2, 1))[:, None, ...]\n",
    "  descriptors2 = img2_feats.view((b, d, -1)).permute((0, 2, 1))[:, None, ...]\n",
    "  return descriptors1, descriptors2\n",
    "\n",
    "def postprocess_points(source_points, predicted_points, output_size, load_size):\n",
    "  source_points = source_points.detach().cpu().numpy()\n",
    "  predicted_points = predicted_points.detach().cpu().numpy()\n",
    "  source_points = rescale_points(source_points, output_size, load_size)\n",
    "  predicted_points = rescale_points(predicted_points, output_size, load_size)\n",
    "  return source_points, predicted_points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute semantic keypoint matches.** Use one of the correspondence functions:\n",
    "- `nearest_neighbors`: For a set of annotated source_points, compute the nearest neighbor predicted_points.\n",
    "- `best_buddies`: Compute the top-k pairs of source_points, predicted_points using the [best buddies algorithm](https://github.com/ShirAmir/dino-vit-features) (Amir et. al., ECCVW 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"assets/spair/images\"\n",
    "saliency_path = \"assets/spair/odise\"\n",
    "annotation_path = \"spair.json\"\n",
    "load_size = (224, 224)\n",
    "output_size = (config[\"output_resolution\"], config[\"output_resolution\"])\n",
    "\n",
    "# Select from [nearest_neighbors, best_buddies]\n",
    "correspondence_function = \"nearest_neighbors\"\n",
    "# Number of correspondences to display when using [best_buddies]\n",
    "num_pairs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/ra63des/DistillDIFT/replicate/luo/archs/aggregation_network.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixing_weights = torch.nn.functional.softmax(self.mixing_weights)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmvl3/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     img2_hyperfeats \u001b[39m=\u001b[39m diffusion_hyperfeats[\u001b[39m1\u001b[39m][\u001b[39mNone\u001b[39;00m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmvl3/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m correspondence_function \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest_neighbors\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmvl3/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m   _, predicted_points \u001b[39m=\u001b[39m find_nn_source_correspondences(img1_hyperfeats, img2_hyperfeats, source_points, output_size, load_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmvl3/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m   predicted_points \u001b[39m=\u001b[39m predicted_points\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmvl3/export/home/ra63des/DistillDIFT/replicate/luo/real_demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m   distances, _, pck_metric \u001b[39m=\u001b[39m compute_pck(predicted_points, target_points, load_size)\n",
      "File \u001b[0;32m~/DistillDIFT/replicate/luo/archs/correspondence_utils.py:125\u001b[0m, in \u001b[0;36mfind_nn_source_correspondences\u001b[0;34m(img1_feats, img2_feats, source_points, output_size, load_size)\u001b[0m\n\u001b[1;32m    123\u001b[0m img2_feats \u001b[39m=\u001b[39m flatten_feats(img2_feats)\n\u001b[1;32m    124\u001b[0m img1_feats \u001b[39m=\u001b[39m img1_feats[:, source_idx, :]\n\u001b[0;32m--> 125\u001b[0m img1_feats \u001b[39m=\u001b[39m normalize_feats(img1_feats)\n\u001b[1;32m    126\u001b[0m img2_feats \u001b[39m=\u001b[39m normalize_feats(img2_feats)\n\u001b[1;32m    127\u001b[0m sims \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(img1_feats, img2_feats\u001b[39m.\u001b[39mpermute((\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)))\n",
      "File \u001b[0;32m~/DistillDIFT/replicate/luo/archs/correspondence_utils.py:70\u001b[0m, in \u001b[0;36mnormalize_feats\u001b[0;34m(feats)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_feats\u001b[39m(feats):\n\u001b[1;32m     69\u001b[0m     \u001b[39m# (b, w*h, c)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     feats \u001b[39m=\u001b[39m feats \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(feats, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[:, :, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m feats\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from datasets.dataset import ConvertedDataset\n",
    "dataset = ConvertedDataset('/export/group/datasets/SPair-71k/converted.h5')\n",
    "\n",
    "for sample in dataset:\n",
    "  source_image = sample['source_image']\n",
    "  target_image = sample['target_image']\n",
    "  source_points = sample['source_points']\n",
    "  target_points = sample['target_points']\n",
    "\n",
    "  img1, img1_pil = process_image(source_image, res=load_size)\n",
    "  img2, img2_pil = process_image(target_image, res=load_size)\n",
    "  img1, img2 = img1.to(device), img2.to(device)\n",
    "  imgs = torch.cat([img1, img2])\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    with torch.autocast(\"cuda\"):\n",
    "      feats, _ = diffusion_extractor.forward(imgs)\n",
    "      b, s, l, w, h = feats.shape\n",
    "      diffusion_hyperfeats = aggregation_network(feats.float().view((b, -1, w, h)))\n",
    "      img1_hyperfeats = diffusion_hyperfeats[0][None, ...]\n",
    "      img2_hyperfeats = diffusion_hyperfeats[1][None, ...]\n",
    "\n",
    "  if correspondence_function == \"nearest_neighbors\":\n",
    "    _, predicted_points = find_nn_source_correspondences(img1_hyperfeats, img2_hyperfeats, source_points, output_size, load_size)\n",
    "    predicted_points = predicted_points.detach().cpu().numpy()\n",
    "    distances, _, pck_metric = compute_pck(predicted_points, target_points, load_size)\n",
    "    title = f\"Diffusion Hyperfeatures, Nearest Neighbors Matches \\n PCK@0.1: {pck_metric.round(decimals=2)}\"\n",
    "  #elif correspondence_function == \"best_buddies\":\n",
    "  #  descriptors1, descriptors2 = reshape_descriptors(img1_hyperfeats, img2_hyperfeats)\n",
    "  #  saliency_map1, saliency_map2 = load_saliency_pair(ann, output_size, img1_hyperfeats.device, saliency_path)\n",
    "  #  source_points, predicted_points = find_best_buddies_correspondences(descriptors1, descriptors2, saliency_map1, saliency_map2, num_pairs=num_pairs)\n",
    "  #  source_points, predicted_points = postprocess_points(source_points, predicted_points, output_size, load_size)\n",
    "  #  title = \"Diffusion Hyperfeatures, Best Buddies Matches\"\n",
    "  else:\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  draw_correspondences(source_points, predicted_points, img1_pil, img2_pil, title=title)\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
