{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedlin et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('hedlin/utils/')\n",
    "from hedlin.utils.optimize_token import load_ldm, optimize_prompt, run_image_with_tokens_cropped, find_max_pixel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corresponding_point(ldm, src_img, trg_img, src_kpts):\n",
    "    upsample_res = 512\n",
    "    num_steps = 100\n",
    "    noise_level = 10\n",
    "    layers = [0, 1, 2, 3, 4, 5]\n",
    "    device = 'cpu'\n",
    "    lr = 1e-3\n",
    "    num_opt_iterations = 5\n",
    "    sigma = 32\n",
    "    flip_prob = 0.5\n",
    "    crop_percent = 80\n",
    "    num_iterations = 20\n",
    "\n",
    "    # Initialize the estimated keypoints\n",
    "    est_keypoints = -1 * torch.ones_like(src_kpts)\n",
    "    ind_layers = -1 * torch.ones_like(src_kpts).repeat(len(layers), 1, 1)\n",
    "\n",
    "    for j in range(src_kpts.shape[1]):\n",
    "        # Find the text embeddings for the source point\n",
    "        contexts = []\n",
    "        for _ in range(num_opt_iterations):\n",
    "            context = optimize_prompt(ldm, src_img, src_kpts[0, :, j]/512, num_steps=num_steps, device=device, layers=layers, lr = lr, upsample_res=upsample_res, noise_level=noise_level, sigma = sigma, flip_prob=flip_prob, crop_percent=crop_percent)\n",
    "            contexts.append(context)\n",
    "\n",
    "        # Find and combine the attention maps over the multiple found text embeddings and crops\n",
    "        all_maps = []\n",
    "        for context in contexts:\n",
    "            maps = []\n",
    "            attn_maps, _ = run_image_with_tokens_cropped(ldm, trg_img, context, index=0, upsample_res = upsample_res, noise_level=noise_level, layers=layers, device=device, crop_percent=crop_percent, num_iterations=num_iterations, image_mask = None)\n",
    "            for k in range(attn_maps.shape[0]):\n",
    "                avg = torch.mean(attn_maps[k], dim=0, keepdim=True)\n",
    "                maps.append(avg)\n",
    "                _max_val = find_max_pixel_value(avg[0], img_size = 512)\n",
    "                ind_layers[k, :, j] = (_max_val+0.5)\n",
    "            maps = torch.stack(maps, dim=0)\n",
    "            all_maps.append(maps)\n",
    "        all_maps = torch.stack(all_maps, dim=0)\n",
    "        all_maps = torch.mean(all_maps, dim=0)\n",
    "        all_maps = torch.nn.Softmax(dim=-1)(all_maps.reshape(len(layers), upsample_res*upsample_res))\n",
    "        all_maps = all_maps.reshape(len(layers), upsample_res, upsample_res)\n",
    "\n",
    "        # Take the argmax to find the corresponding location for the target image\n",
    "        all_maps = torch.mean(all_maps, dim=0)\n",
    "        max_val = find_max_pixel_value(all_maps, img_size = 512)\n",
    "        est_keypoints[0, :, j] = (max_val+0.5)\n",
    "\n",
    "    return est_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 167548.76it/s]\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "ldm = load_ldm('cpu', 'CompVis/stable-diffusion-v1-4')\n",
    "ldm.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/Library/Python/3.9/lib/python/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m point1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0.4\u001b[39m, \u001b[39m0.9\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m point1 \u001b[39m=\u001b[39m point1\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m512.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m point2 \u001b[39m=\u001b[39m find_corresponding_point(ldm, img1, img2, point1\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[1;32m/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m contexts \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_opt_iterations):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     context \u001b[39m=\u001b[39m optimize_prompt(ldm, src_img, src_kpts[\u001b[39m0\u001b[39;49m, :, j]\u001b[39m/\u001b[39;49m\u001b[39m512\u001b[39;49m, num_steps\u001b[39m=\u001b[39;49mnum_steps, device\u001b[39m=\u001b[39;49mdevice, layers\u001b[39m=\u001b[39;49mlayers, lr \u001b[39m=\u001b[39;49m lr, upsample_res\u001b[39m=\u001b[39;49mupsample_res, noise_level\u001b[39m=\u001b[39;49mnoise_level, sigma \u001b[39m=\u001b[39;49m sigma, flip_prob\u001b[39m=\u001b[39;49mflip_prob, crop_percent\u001b[39m=\u001b[39;49mcrop_percent)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     contexts\u001b[39m.\u001b[39mappend(context)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Find and combine the attention maps over the multiple found text embeddings and crops\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin/utils/optimize_token.py:522\u001b[0m, in \u001b[0;36moptimize_prompt\u001b[0;34m(ldm, image, pixel_loc, context, device, num_steps, from_where, upsample_res, layers, lr, noise_level, sigma, flip_prob, crop_percent)\u001b[0m\n\u001b[1;32m    518\u001b[0m controller \u001b[39m=\u001b[39m AttentionStore()\n\u001b[1;32m    520\u001b[0m ptp_utils\u001b[39m.\u001b[39mregister_attention_control(ldm, controller)\n\u001b[0;32m--> 522\u001b[0m _ \u001b[39m=\u001b[39m ptp_utils\u001b[39m.\u001b[39;49mdiffusion_step(ldm, controller, noisy_image, context, ldm\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mtimesteps[noise_level], cfg \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    524\u001b[0m attention_maps \u001b[39m=\u001b[39m upscale_to_img_size(controller, from_where \u001b[39m=\u001b[39m from_where, upsample_res\u001b[39m=\u001b[39mupsample_res, layers \u001b[39m=\u001b[39m layers)\n\u001b[1;32m    525\u001b[0m num_maps \u001b[39m=\u001b[39m attention_maps\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin/utils/ptp_utils.py:31\u001b[0m, in \u001b[0;36mdiffusion_step\u001b[0;34m(model, controller, latents, context, t, guidance_scale, cfg)\u001b[0m\n\u001b[1;32m     29\u001b[0m     noise_pred \u001b[39m=\u001b[39m noise_pred_uncond \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (noise_prediction_text \u001b[39m-\u001b[39m noise_pred_uncond)\n\u001b[1;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     noise_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49munet(latents, t, encoder_hidden_states\u001b[39m=\u001b[39;49mcontext)[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     33\u001b[0m latents \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(noise_pred, t, latents)[\u001b[39m\"\u001b[39m\u001b[39mprev_sample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m latents \u001b[39m=\u001b[39m controller\u001b[39m.\u001b[39mstep_callback(latents)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/unet_2d_condition.py:313\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mfor\u001b[39;00m downsample_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(downsample_block, \u001b[39m\"\u001b[39m\u001b[39mattentions\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m downsample_block\u001b[39m.\u001b[39mattentions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(\n\u001b[1;32m    314\u001b[0m             hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    315\u001b[0m             temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m    316\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/unet_2d_blocks.py:628\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m--> 628\u001b[0m         hidden_states \u001b[39m=\u001b[39m attn(hidden_states, encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states)\u001b[39m.\u001b[39msample\n\u001b[1;32m    630\u001b[0m     output_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (hidden_states,)\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention.py:203\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, return_dict)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 203\u001b[0m     hidden_states \u001b[39m=\u001b[39m block(hidden_states, context\u001b[39m=\u001b[39;49mencoder_hidden_states, timestep\u001b[39m=\u001b[39;49mtimestep)\n\u001b[1;32m    205\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention.py:451\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, context, timestep)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39m# 2. Cross-Attention\u001b[39;00m\n\u001b[1;32m    448\u001b[0m norm_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m    449\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states, timestep) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ada_layer_norm \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states)\n\u001b[1;32m    450\u001b[0m )\n\u001b[0;32m--> 451\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn2(norm_hidden_states, context\u001b[39m=\u001b[39;49mcontext) \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    453\u001b[0m \u001b[39m# 3. Feed-forward\u001b[39;00m\n\u001b[1;32m    454\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(hidden_states)) \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/Distill-DIFT/replicate/hedlin/utils/ptp_utils.py:174\u001b[0m, in \u001b[0;36mregister_attention_control.<locals>.ca_forward.<locals>.forward\u001b[0;34m(x, context, mask)\u001b[0m\n\u001b[1;32m    172\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    173\u001b[0m attn \u001b[39m=\u001b[39m controller(attn, is_cross, place_in_unet)\n\u001b[0;32m--> 174\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn, v)\n\u001b[1;32m    176\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_batch_dim_to_heads(out)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m to_out(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_image(img_name):\n",
    "    image = Image.open(img_name).convert('RGB')\n",
    "    image = image.resize((512, 512), Image.BILINEAR)\n",
    "    image = np.array(image)\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image = torch.tensor(image)/255.0\n",
    "    return image\n",
    "        \n",
    "img1 = load_image('hedlin/assets/source_cat.png')\n",
    "img2 = load_image('hedlin/assets/target_cat.jpeg')\n",
    "point1 = torch.tensor([[0.4, 0.9]])\n",
    "point1 = point1.permute(1, 0) * 512.0\n",
    "point2 = find_corresponding_point(ldm, img1, img2, point1.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
