# Configuration file for the models
# See README.md for more details

############ Training ############

distilled_s:
  image_size: [336, 336]
  image_range: [0, 1]
  weights: null
  rank: 8
  lora_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
  linear_head: false

  batch_size: 1
  num_epochs: 25
  learning_rate: 0.0001 # 0.0001 for LoRA and linear head, 0.00001 for full fine-tune, /10 for softmax with cross-entropy
  gradient_accumulation_steps: 1
  mode: "train"
  scheduler_type: "constant"
  similarity_method: "soft_argmax"
  loss_function: "mse"
  half_precision: false

############ Distillation ############

distilled_ws:
  image_size: [434, 434] #[336, 336]
  image_range: [0, 1]

  teacher_name: "combination_add"
  teacher_config:
    model1: 'diff_add'
    model2: 'dinov2_b14_reg'
    model1_config:
      model: "stabilityai/sdxl-turbo"
      layers: [1]
      step: 101
    model2_config:
      version: 2
      model_size: 'b'
      patch_size: 14
      registers: true
      layers: [11]

  student_name: "distilled_model"
  student_config:
    weights: null
    rank: 8
    lora_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
    lora_dropout: 0.05
    linear_head: false

  batch_size: 1
  num_epochs: 1
  learning_rate: 0.0001
  weight_decay: 0.05
  softmax_temperature: 0.01
  softargmax_beta: 1000.0
  step_percent: 0.5

  mode: "distill"
  scheduler_type: "step"
  sampling_method: "full"
  similarity_method: "softmax"
  loss_function: "cross_entropy"
  half_precision: false
