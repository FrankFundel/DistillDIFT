# DistillDIFT
Distilling the capability of large diffusion models for semantic correspondence.

# ToDo
1. Replicate Hedlin et al.: https://github.com/ubc-vision/LDM_correspondences
2. Replicate Tang et al.: https://github.com/Tsingularity/dift
3. Replicate Zhang et al.: https://github.com/Junyi42/sd-dino
4. Replicate Luo et al.: https://github.com/diffusion-hyperfeatures/diffusion_hyperfeatures

# Evaluation

1. Download datasets
    - SPair-71k: `bash datasets/download_spair.sh`
    - PF-WILLOW: `bash datasets/download_pfwillow.sh`
2. Convert datasets to HDF5